; Configuration file for executables, settings common to OSG and local running
;
[executables]
average_psd = ${which:pycbc_average_psd}
inj2hdf = ${which:pycbc_convertinjfiletohdf}
bank2hdf = ${which:pycbc_coinc_bank2hdf}
bayestar = ${which:pycbc_make_bayestar_skymap}
calculate_psd = ${which:pycbc_calculate_psd}
coinc = ${which:pycbc_coinc_findtrigs}
combine_statmap = ${which:pycbc_add_statmap}
exclude_zerolag = ${which:pycbc_exclude_zerolag}
fit_by_template = ${which:pycbc_fit_sngls_by_template}
fit_over_param = ${which:pycbc_fit_sngls_over_multiparam}
foreground_censor = ${which:pycbc_foreground_censor}
generate_xml = ${which:pycbc_prepare_xml_for_gracedb}
hdfinjfind = ${which:pycbc_coinc_hdfinjfind}
hdf_trigger_merge = ${which:pycbc_coinc_mergetrigs}
inj_cut = ${which:pycbc_inj_cut}
injections = ${which:lalapps_inspinj}
ligolw_combine_segments = ${which:ligolw_combine_segments}
merge_psds = ${which:pycbc_merge_psds}
optimal_snr = ${which:pycbc_optimal_snr}
optimal_snr_merge = ${which:pycbc_merge_inj_hdf}
page_foreground = ${which:pycbc_page_foreground}
page_ifar = ${which:pycbc_page_ifar}
page_ifar_catalog = ${which:pycbc_ifar_catalog}
page_injections = ${which:pycbc_page_injtable}
page_segplot = ${which:pycbc_page_segplot}
page_segtable = ${which:pycbc_page_segtable}
page_versioning = ${which:pycbc_page_versioning}
page_vetotable = ${which:pycbc_page_vetotable}
plot_bank = ${which:pycbc_plot_bank_bins}
plot_binnedhist = ${which:pycbc_fit_sngls_split_binned}
plot_coinc_snrchi = ${which:pycbc_page_coinc_snrchi}
plot_foundmissed = ${which:pycbc_page_foundmissed}
plot_gating = ${which:pycbc_plot_gating}
plot_hist = ${which:pycbc_plot_hist}
plot_qscan = ${which:pycbc_plot_qscan}
plot_range = ${which:pycbc_plot_range}
plot_segments = ${which:pycbc_page_segments}
plot_sensitivity = ${which:pycbc_page_sensitivity}
plot_singles = ${which:pycbc_plot_singles_vs_params}
plot_snrchi = ${which:pycbc_page_snrchi}
plot_snrifar = ${which:pycbc_page_snrifar}
plot_spectrum = ${which:pycbc_plot_psd_file}
plot_throughput = ${which:pycbc_plot_throughput}
results_page = ${which:pycbc_make_html_page}
skymap_plot = ${which:ligo-skymap-plot}
sngls = ${which:pycbc_sngls_findtrigs}
sngls_statmap = ${which:pycbc_sngls_statmap}
sngls_statmap_inj = ${which:pycbc_sngls_statmap_inj}
splitbank = ${which:pycbc_hdf5_splitbank}
statmap = ${which:pycbc_coinc_statmap}
statmap_inj = ${which:pycbc_coinc_statmap_inj}
strip_injections = ${which:pycbc_strip_injections}


; ################### DQ Executables ##########################
;calculate_dq = ${which:pycbc_calculate_dq}
;calculate_dqflag = ${which:pycbc_calculate_dqflag}
;rerank_dq = ${which:pycbc_rerank_dq}
;bin_trigger_rates_dq = ${which:pycbc_bin_trigger_rates_dq}
;plot_dq_likelihood_vs_time = ${which:pycbc_plot_dq_likelihood_vs_time}
;plot_dq_percentiles = ${which:pycbc_plot_dq_percentiles}
;plot_dq_flag_likelihood = ${which:pycbc_plot_dq_flag_likelihood}

; #################### Mini Followup #########################################
;foreground_minifollowup = ${which:pycbc_foreground_minifollowup}
;injection_minifollowup = ${which:pycbc_injection_minifollowup}
;singles_minifollowup = ${which:pycbc_sngl_minifollowup}
;upload_prep_minifollowup = ${which:pycbc_upload_prep_minifollowup}

tmpltbank = ${which:pycbc_geom_nonspinbank}
html_snippet = ${which:pycbc_create_html_snippet}
;page_injinfo = ${which:pycbc_page_injinfo}
page_coincinfo = ${which:pycbc_page_coincinfo}
;page_snglinfo = ${which:pycbc_page_snglinfo}
plot_trigger_timeseries = ${which:pycbc_plot_trigger_timeseries}
;single_template_plot = ${which:pycbc_single_template_plot}
;single_template = ${which:pycbc_single_template}
;plot_singles_timefreq = ${which:pycbc_plot_singles_timefreq}
plot_snrratehist = ${which:pycbc_page_snrratehist}
;plot_waveform = ${which:pycbc_plot_waveform}



; #################### Executable Memory Requirements ########################

[pegasus_profile]
; This section contains default profile information for every job
; This is overriden by profile information set for specific job types

; This sets the initial memory footprint request, initial disk request and
; CPU request
; This is in MB
condor|+InitialRequestMemory = 1980
; This is in KB, take care!!
condor|+InitialRequestDisk = 100000
condor|request_cpus = 1
; Jobs will be evicted after running for this long. We then try again with
; longer runtimes progressively to try and get the job to complete. This
; catches the common case that a job gets "stuck" but does allow long-running
; jobs to complete, albeit with more delay.
condor|+ExpectedMaxRunTime = 20000
; Use this to override the number of times a job is allowed to start before
; evicting it. If memory/disk requests are way too low, a job may have to
; start multiple times before completing
condor|+MaxJobStarts = 5
; Use the initial request unless the job has been evicted for using too much
; memory. In that case, ask for 50% more than the last resident memory use.
; If it wasn't evicted for too much memory we set the memory request to the
; previous usage. If this job is running for the first time, MemoryUsage is not
; defined and we use InitialRequestMemory, we also use InitialRequestMemory
; if it is larger than previous MemoryUsage. (Sorry this is convoluted!)
; This uses an intermediate variable for readability
condor|+MinimumMemoryNeeded = max({ifThenElse((isInteger(NumJobStarts) && (NumJobStarts > 0) && (MemoryUsage =!= UNDEFINED) && (MemoryUsage =!= ERROR)), MemoryUsage, 0), InitialRequestMemory})
condor|request_memory = ifThenElse( ((LastHoldReasonCode=?=21) || (LastHoldReasonCode=?=34)), int(1.5 * MemoryUsage), MinimumMemoryNeeded)
; request_disk is a simpler expression. Please let Ian know if we have issues
; with this. It's not always clear that DiskUsage is defined when the check at
; submission time evaluates this.
condor|request_disk = int(1.5 * DiskUsage + InitialRequestDisk)
; If the inspiral job has run for more than the expected max run time
; multiplied by the number of job starts, assume something is stuck and evict
; it.
condor|periodic_hold = (JobStatus =?= 2) && ((CurrentTime - EnteredCurrentStatus) > (ExpectedMaxRunTime*NumJobStarts + 600))
; If the job has been held for understood reasons, release it and let the
; memory request bump or disk request bump take effect if needed
; For codes see here:
; https://htcondor.readthedocs.io/en/latest/classad-attributes/job-classad-attributes.html#HoldReasonCode
;
; 3 = The periodic hold condition was met.
; 21 = The job was put on hold by condor itself for some reason.
; 26 = CIT is using this code for exceeding memory usage.
; 32 and 33 = The job exceeded disk usage (possibly!)
; 34 = Job exceeded memory limit
condor|periodic_release = ((HoldReasonCode =?= 26) || (HoldReasonCode =?= 21) || (HoldReasonCode =?= 34) || (HoldReasonCode =?= 32) || (HoldReasonCode =?= 33) || ((JobStatus =?= 5) && (HoldReasonCode =?= 3) && (NumJobStarts < MaxJobStarts))) && ((CurrentTime - EnteredCurrentStatus) > (300))
; If held for these reasons remove the job. This logs a failure, which we
; sometimes want, and then goes to our standard retry options.
; 12/13 = Issues during condor transfer. Will hopefully reduce these by using
;         other settings to raise failures in these cases.
condor|periodic_remove = ((HoldReasonCode =?= 12) || (HoldReasonCode =?= 13) || (NumJobStarts >= 5)) && ((CurrentTime - EnteredCurrentStatus) > (300))

pycbc|installed = True
condor|accounting_group = ligo.dev.o4.cbc.grb.cohptfoffline
pycbc|primary_site = condorpool_symlink


; WARNING: If another site is used, it probably needs adding here!
[pegasus_profile-condorpool_shared&pegasus_profile-condorpool_symlink&pegasus_profile-osg]
; This is the lines we need for scitokens
;condor|use_oauth_services = igwn
condor|use_oauth_services = scitokens
;testing further auth stuff
;condor|igwn_oauth_resource_datafind = https://datafind.ligo.org
;condor|igwn_oauth_permissions_datafind = read:/frames


; This line is needed to find the credential on condor
;env|BEARER_TOKEN_FILE = $$(CondorScratchDir)/.condor_creds/igwn.use
env|BEARER_TOKEN_FILE=$$(CondorScratchDir)/.condor_creds/scitokens.use
; Always disable HDF file locking (though only important on condorpool_symlink)
env|HDF5_USE_FILE_LOCKING = FALSE
; Always set the PATH to the ROM data files. Should be changed once waveforms
; folk start managing this properly. For now Ian has access to this location,
; though it is not ideal for storing these files
env|LAL_DATA_PATH=/cvmfs/software.igwn.org/pycbc/lalsuite-extra/e02dab8c/share/lalsimulation

[pegasus_profile-condorpool_shared&pegasus_profile-condorpool_symlink]
; On local clusters GWDATAFIND_SERVER comes from local environment
env|GWDATAFIND_SERVER = ${environment|GWDATAFIND_SERVER}

[pegasus_profile-osg]
; Explicitly ensure the correct datafind server is set on OSG
env|GWDATAFIND_SERVER=datafind.ligo.org:443


[pegasus_profile-condorpool_shared]

[pegasus_profile-condorpool_symlink]

; #################### Pegasus Configuration for Executables ##################
[pegasus_profile-results_page]
pycbc|site = condorpool_shared

[pegasus_profile-calculate_psd]
condor|+InitialRequestMemory = 8000
condor|+InitialRequestDisk = 550000
condor|request_cpus = ${calculate_psd|cores}
dagman|priority = 5000
dagman|retry = 10

;[pegasus_profile-injections]
;dagman|priority = 700

[pegasus_profile-bank2hdf]
dagman|priority = 5000

[pegasus_profile-merge_psds]
dagman|priority = 2000
condor|+InitialRequestDisk = 5200000

[pegasus_profile-ligolw_combine_segments]
dagman|priority = 5000

[pegasus_profile-llwadd]
dagman|priority = 5000

[pegasus_profile-splitbank]
dagman|priority = 5000

[pegasus_profile-foreground_censor]
dagman|priority = 5000

;[pegasus_profile-foreground_minifollowup]
;dagman|priority = 5000

;[pegasus_profile-injection_minifollowup]
;dagman|priority = 5000

[pegasus_profile-coinc]
dagman|priority = 5000
condor|+InitialRequestMemory = 5000
condor|+InitialRequestDisk = 200000

[pegasus_profile-sngls]
dagman|priority = 5000
condor|+InitialRequestMemory = 5000
condor|+InitialRequestDisk = 150000

[pegasus_profile-fit_by_template]
dagman|priority = 5000
condor|+InitialRequestMemory = 40000

[pegasus_profile-hdf_trigger_merge]
dagman|priority = 5000
condor|+InitialRequestMemory = 40000
condor|+InitialRequestDisk = 10000000

[pegasus_profile-inspiral]
condor|+InitialRequestDisk = 2000000
condor|+InitialRequestMemory = 9000

; Removing this temporarily 5-16-24
;[pegasus_profile-optimal_snr]
;condor|request_cpus = ${optimal_snr|cores}
;condor|+InitialRequestMemory = 10000
;dagman|priority = 5000

;[pegasus_profile-page_snglinfo]
;condor|+InitialRequestMemory = 4000
# This option will cause pegasus to cluster these jobs together in groups of
# 5 jobs. This means that 5 page_snglinfo jobs will become one condor job.
# That condor job will run the 5 jobs in sequence. Pegasus will keep track
# of which jobs are complete on eviction (or similar) to avoid repetition.
;pegasus|clusters.size = 5

[pegasus_profile-page_coincinfo]
pegasus|clusters.size = 5
condor|+InitialRequestMemory = 9000

[pegasus_profile-page_foreground]
condor|+InitialRequestMemory = 30000

[pegasus_profile-plot_waveform]
pegasus|clusters.size = 5

;[pegasus_profile-plot_singles_timefreq]
;pegasus|clusters.size = 5

;[pegasus_profile-plot_qscan]
;pegasus|clusters.size = 5

[pegasus_profile-plot_binnedhist]
condor|+InitialRequestMemory = 30000

[pegasus_profile-plot_coinc_snrchi]
condor|+InitialRequestMemory = 31600

[pegasus_profile-plot_hist]
condor|+InitialRequestMemory = 30000

;[pegasus_profile-plot_singles]
;condor|+InitialRequestMemory = 40000

;[pegasus_profile-plot_snrchi]
;condor|+InitialRequestMemory = 20000

[pegasus_profile-plot_spectrum]
condor|+InitialRequestMemory = 3000

[pegasus_profile-plot_trigger_timeseries]
condor|+InitialRequestMemory = 10000
pegasus|clusters.size = 5

;[pegasus_profile-singles_minifollowup]
;condor|+InitialRequestMemory = 20000
;dagman|priority = 500

;[pegasus_profile-single_template]
;condor|+InitialRequestMemory = 9000
;pegasus|clusters.size = 5
;condor|+InitialRequestDisk = 500000

;[pegasus_profile-single_template_plot]
;condor|+InitialRequestMemory = 9000
;pegasus|clusters.size = 5


;[pegasus_profile-upload_prep_minifollowup]
;dagman|priority = 500

[pegasus_profile-generate_xml]
condor|+InitialRequestMemory = 3000
pegasus|clusters.size = 5

[pegasus_profile-bayestar]
condor|+InitialRequestMemory = 3000
pegasus|clusters.size = 5

[pegasus_profile-skymap_plot]
condor|+InitialRequestMemory = 3000
pegasus|clusters.size = 5


[pegasus_profile-statmap]
condor|+InitialRequestMemory = 15000
dagman|priority = 500

;[pegasus_profile-statmap_inj]
;condor|+InitialRequestMemory = 10000
;dagman|priority = 500

[pegasus_profile-sngls_statmap]
condor|+InitialRequestMemory = 30000
dagman|priority = 500

;[pegasus_profile-sngls_statmap_inj]
;condor|+InitialRequestMemory = 10000
;dagman|priority = 500

[pegasus_profile-exclude_zerolag]
condor|+InitialRequestMemory = 9000
condor|+InitialRequestDisk = 400000
dagman|priority = 500

[pegasus_profile-combine_statmap]
condor|+InitialRequestMemory = 9000
condor|+InitialRequestDisk = 400000
dagman|priority = 500

;[pegasus_profile-hdfinjfind]
;condor|+InitialRequestMemory = 5000
;dagman|priority = 500

[pegasus_profile-bin_trigger_rates_dq]
dagman|priority = 5000
condor|+InitialRequestMemory = 40000

[pegasus_profile-rerank_dq]
dagman|priority = 5000
